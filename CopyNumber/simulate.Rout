
R version 4.3.3 (2024-02-29) -- "Angel Food Cake"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> library(patchwork)
> library(loo)
This is loo version 2.8.0
- Online documentation and vignettes at mc-stan.org/loo
- As of v2.0.0 loo defaults to 1 core but we recommend using as many as possible. Use the 'cores' argument or set options(mc.cores = NUM_CORES) for an entire session. 
> library(bayesplot)
This is bayesplot version 1.11.1
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
> library(cmdstanr)
This is cmdstanr version 0.8.1
- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr
- CmdStan path: /u/cdslab/scocomello/.cmdstan/cmdstan-2.34.1
- CmdStan version: 2.34.1

A newer version of CmdStan is available. See ?install_cmdstan() to install it.
To disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.
> library(factoextra)
Loading required package: ggplot2
Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> 
> 
> #setwd("C:/Users/sarac/CDS_git/Copy-Number-Timing/CopyNumber/")
> #orfeo
> setwd("/orfeo/cephfs/scratch/cdslab/scocomello/Copy_Number_Timing/CopyNumber")
> 
> original_dir <- getwd()
> 
> source("./CNTiming/R/simulate_functions.R")
> source("./CNTiming/R/fitting_functions.R")
> source("./CNTiming/R/plotting_functions.R")
> 
> number_events = 3  #3, 10
> number_clocks = 3
> 
> INIT = FALSE
> epsilon = 0.20
> n_simulations = 1
> purity = 0.99
> 
> vector_karyo <- c("2:0", "2:1", "2:2")
> weights_karyo <- c(0.33, 0.33, 0.33)
> 
> options(bitmapType='cairo')
> 
> 
> for(i in 1:n_simulations){
+   # Create a unique directory for each iteration
+   iter_dir <- paste0("./simulation_iteration_", i)
+   dir.create(iter_dir)
+   setwd(iter_dir)
+   dir.create(paste0("./plots"), showWarnings = TRUE)
+   dir.create(paste0("./results"), showWarnings = FALSE)
+   
+   
+ 
+   vector_tau = rep(0, number_clocks)
+   
+   for (j in 1:number_clocks){
+     vector_tau[j] = runif(1, 0)
+     if (j != 1){
+       while (!all ( abs(vector_tau[1:j-1] - vector_tau[j]) > epsilon  )   ){
+         vector_tau[j] = runif(1, 0)
+       }
+     }
+   }
+   weights_tau <- rep(1/number_clocks, number_clocks)
+   
+   data <- get_taus_karyo(number_events, vector_tau, vector_karyo, weights_tau, weights_karyo)
+   all_sim = get_simulation(data$taus, data$karyo, purity)
+   data_sim <- all_sim
+   #add statistics on number of mutations from the simulation
+   
+   plot_data <- data_sim %>%
+     ggplot(mapping = aes(x = NV / DP, fill = as.factor(j))) +
+     geom_histogram(alpha = .5, position = "identity") +
+     facet_wrap(vars(karyotype, tau, j))
+   
+   ggsave("./plots/original_data.png", plot = plot_data, width = 12, height = 10,   device = png)
+   # device = function(...) png(..., type = "cairo")
+ 
+   simulation_params <- list(
+     vector_tau = vector_tau,
+     vector_karyo = vector_karyo,
+     weights_tau = weights_tau,
+     weights_karyo = weights_karyo,
+     taus = data$taus,
+     karyo = data$karyo,
+     purity = purity,
+     number_events = number_events,
+     number_clocks = number_clocks,
+     epsilon = epsilon
+   )
+   
+   
+   #in "fit model selection best K" the plots for each K inference is directly saved 
+   results <- fit_model_selection_best_K(data_sim, data$karyo, purity, INIT = INIT, simulation_params = simulation_params)
+   
+   
+   
+   
+   
+   
+   
+   model_selection <- results$model_selection_tibble
+   best_K <- results$best_K
+   
+   
+   bic_plot <- ggplot(data = model_selection, aes(x = K, y = BIC)) + 
+     geom_line(aes(colour = "BIC Line")) + 
+     geom_point(aes(colour = "BIC Point")) +
+     geom_point(aes(x = best_K, y = BIC[K == best_K], colour = "Best K Point")) +
+     geom_point(aes(x = K[BIC == min(BIC)], y = min(BIC), colour = "Minimum Point")) +
+     scale_colour_manual(name = "Legend",
+                         values = c("BIC Line" = "blue", 
+                                    "BIC Point" = "blue", 
+                                    "Best K Point" = "red",
+                                    "Minimum Point" = "green"),)
+   
+   aic_plot <- ggplot(data = model_selection, aes(x = K, y = AIC)) + 
+     geom_line(aes(colour = "AIC Line")) + 
+     geom_point(aes(colour = "AIC Point")) +
+     geom_point(aes(x = best_K, y = AIC[K == best_K], colour = "Best K Point")) +
+     geom_point(aes(x = K[AIC == min(AIC)], y = min(AIC), colour = "Minimum Point")) +
+     scale_colour_manual(name = "Legend",
+                         values = c("AIC Line" = "blue", 
+                                    "AIC Point" = "blue", 
+                                    "Best K Point" = "red",
+                                    "Minimum Point" = "green"))
+   
+ 
+   loo_plot <- ggplot(data = model_selection, aes(x = K, y = LOO)) + 
+     geom_line(aes(colour = "LOO Line")) + 
+     geom_point(aes(colour = "LOO Point")) +
+     geom_point(aes(x = best_K, y = LOO[K == best_K], colour = "Best K Point")) +
+     geom_point(aes(x = K[LOO == min(LOO)], y = min(LOO), colour = "Minimum Point")) +
+     scale_colour_manual(name = "Legend",
+                         values = c("LOO Line" = "blue", 
+                                    "LOO Point" = "blue", 
+                                    "Best K Point" = "red",
+                                    "Minimum Point" = "green"))
+   
+   
+   log_lik_plot <- ggplot(data = model_selection, aes(x = K, y = Log_lik)) + 
+     geom_line(aes(colour = "Log likelihood Line")) + 
+     geom_point(aes(colour = "Log likelihood Point")) +
+     geom_point(aes(x = best_K, y = Log_lik[K == best_K], colour = "Best K Point")) +
+     geom_point(aes(x = K[Log_lik == max(Log_lik)], y = max(Log_lik), colour = "Minimum Point")) +
+     scale_colour_manual(name = "Legend",
+                         values = c("Log likelihood Line" = "blue", 
+                                    "Log likelihood Point" = "blue", 
+                                    "Best K Point" = "red",
+                                    "Minimum Point" = "green"))
+   
+   
+   
+   Subtitle <- vector("list", S)
+   
+   for (i in 1:S) {
+     n_parameters <- i+(i*S)+2
+     Subtitle[[i]] <- paste0(" ", i, ": ", n_parameters," parameters")
+   }
+   
+   Subtitle <- paste(Subtitle, collapse = "\n")
+   
+   
+   
+   
+   model_selection_plot <- (bic_plot | aic_plot) / (loo_plot|log_lik_plot) +
+   plot_annotation(
+     title = "Model selection graphs: score vs number of clusters" ,
+     subtitle = paste0 ("Correspondence between number of clusters and number of parameters:  \n", Subtitle),
+     caption = "caption"
+   ) & theme(text = element_text(size = 8), plot.title = element_text(size = 10), plot.subtitle = element_text(size = 8), axis.text = element_text(size = 8), plot.caption = element_text(size = 5))
+   
+   
+   
+   
+   ggsave("./plots/model_selection_plot.png", plot = model_selection_plot, width = 12, height = 10,  device = png)
+   saveRDS(model_selection, "model_selection.rds")
+   
+   
+   
+   setwd(original_dir)
+   
+ }
[1] 0.9369559
[1] 0.6427554
[1] 0.6427554
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
Attempt 1 with 10000 iterations, 1 grad_samples, and 100 elbo_samples
------------------------------------------------------------ 
EXPERIMENTAL ALGORITHM: 
  This procedure has not been thoroughly tested and may be unstable 
  or buggy. The interface is subject to change. 
------------------------------------------------------------ 
Gradient evaluation took 0.00101 seconds 
1000 transitions using 10 leapfrog steps per transition would take 10.1 seconds. 
Adjust your expectations accordingly! 
Begin eta adaptation. 
Iteration:   1 / 250 [  0%]  (Adaptation) 
Iteration:  50 / 250 [ 20%]  (Adaptation) 
Iteration: 100 / 250 [ 40%]  (Adaptation) 
Iteration: 150 / 250 [ 60%]  (Adaptation) 
Iteration: 200 / 250 [ 80%]  (Adaptation) 
Success! Found best value [eta = 1] earlier than expected. 
Begin stochastic gradient ascent. 
  iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes  
   100        -3071.921             1.000            1.000 
   200        -3071.389             0.500            1.000 
   300        -3071.859             0.333            0.000   MEDIAN ELBO CONVERGED 
Drawing a sample of size 1000 from the approximate posterior...  
COMPLETED. 
Finished in  2.5 seconds.
Fit succeeded in attempt 1
Inference completed successfully.
Scale for x is already present.
Adding another scale for x, which will replace the existing scale.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
Attempt 1 with 10000 iterations, 1 grad_samples, and 100 elbo_samples
------------------------------------------------------------ 
EXPERIMENTAL ALGORITHM: 
  This procedure has not been thoroughly tested and may be unstable 
  or buggy. The interface is subject to change. 
------------------------------------------------------------ 
Gradient evaluation took 0.000812 seconds 
1000 transitions using 10 leapfrog steps per transition would take 8.12 seconds. 
Adjust your expectations accordingly! 
Begin eta adaptation. 
Iteration:   1 / 250 [  0%]  (Adaptation) 
Iteration:  50 / 250 [ 20%]  (Adaptation) 
Iteration: 100 / 250 [ 40%]  (Adaptation) 
Iteration: 150 / 250 [ 60%]  (Adaptation) 
Iteration: 200 / 250 [ 80%]  (Adaptation) 
Success! Found best value [eta = 1] earlier than expected. 
Begin stochastic gradient ascent. 
  iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes  
   100        -3073.336             1.000            1.000 
   200        -3072.906             0.500            1.000 
   300        -3074.166             0.334            0.000   MEDIAN ELBO CONVERGED 
Drawing a sample of size 1000 from the approximate posterior...  
COMPLETED. 
Finished in  2.6 seconds.
Fit succeeded in attempt 1
Inference completed successfully.
Scale for x is already present.
Adding another scale for x, which will replace the existing scale.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`geom_line()`: Each group consists of only one observation.
ℹ Do you need to adjust the group aesthetic?
`geom_line()`: Each group consists of only one observation.
ℹ Do you need to adjust the group aesthetic?
`geom_line()`: Each group consists of only one observation.
ℹ Do you need to adjust the group aesthetic?
`geom_line()`: Each group consists of only one observation.
ℹ Do you need to adjust the group aesthetic?
Warning messages:
1: In parse_stancsv_comments(comments) :
  line with "Elapsed Time" not found
2: In parse_stancsv_comments(comments) :
  line with "Elapsed Time" not found
3: In parse_stancsv_comments(comments) :
  line with "Elapsed Time" not found
> 
> 
> proc.time()
   user  system elapsed 
 45.786   4.232  59.768 
